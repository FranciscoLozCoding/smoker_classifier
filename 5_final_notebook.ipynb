{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b66d4dd7-b941-4b8f-8556-79b3d0e52ef8",
   "metadata": {
    "id": "b66d4dd7-b941-4b8f-8556-79b3d0e52ef8"
   },
   "source": [
    "# Final Notebook for DSC478 Project\n",
    "Names: Matt Soria, Jack Leniart, Francisco Lozano  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86be7965-b74f-4da7-ad0f-962d7d2b1ffb",
   "metadata": {
    "id": "86be7965-b74f-4da7-ad0f-962d7d2b1ffb"
   },
   "source": [
    "## Table of Contents\n",
    "* [Overview](#overiew)  \n",
    "* [Task 1 - Preprocessing](#task1)  \n",
    "* [Task 2 - Exploratory Data Analysis](#task2)  \n",
    "* [Task 3 - Benchmark Model](#task3)  \n",
    "* [Task 4 - Models](#task4)\n",
    "    * [Xgboost](#xgboost)\n",
    "    * [Knn](#knn)\n",
    "    * [SVM](#svm)\n",
    "    * [Evaluation](#evaluation)\n",
    "* [Task 5 - Application](#task5)\n",
    "* [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b306a1d-a3b6-49f8-b1b5-93c6df90f013",
   "metadata": {
    "id": "3b306a1d-a3b6-49f8-b1b5-93c6df90f013"
   },
   "source": [
    "## Overview <a class=\"anchor\" id=\"overview\"></a>\n",
    "This notebook includes both our Executive Summary (covered in the Overview and Conclusion) - as well as our final report. This was approved by the professor.  \n",
    "\n",
    "Note: If opening the html file - it won't display the images while running the jupyter - open the html file outside of jupyter and make sure the images folder is in the same directory as the .html file.  \n",
    "\n",
    "For our project we decided to create a classifer for a kaggle dataset that has bio-signals along with information on whether individuals are smokers or non-smokers. Our classifiers will attempt to identify whether or not a patient is a smoker based on the 22 features provided.\n",
    "(https://www.kaggle.com/datasets/gauravduttakiit/smoker-status-prediction-using-biosignals?resource=download&select=train_dataset.csv).  \n",
    "\n",
    "We originally chose this dataset because we recognized the potential ability to build a model that would be used in healthcare.  One application of our model could be used by epidemiologists - attempting to understand population health trends/information - without directly having access to whether or not a person was a smoker.   \n",
    "\n",
    "We created 5 tasks in order to build the classifiers, select the best, and create an application that would allow user's to interact with the model.  \n",
    "* Task 1 - Preprocessing\n",
    "* Task 2 - Exploratory Analysis\n",
    "* Task 3 - Benchmark Model\n",
    "* Task 4 - Models\n",
    "* Task 5 - Application\n",
    "\n",
    "We stuck pretty close to our proposal - however during model evaluation Francisco decided to test if combining all of our models together as an ensemble to see if we'd get better results. After putting this together we were able to confirm the results did improve with this approach. So instead of selecting one best model, we built an ensemble of all three of the models we built.  \n",
    "\n",
    "Finally, we were able to export the ensemble model and build an application that used the ensemble. This would give users, epidemiologists, institutions, etc the ability to use our model and predict whether or not a patient is a smoker.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd95ec2-6a59-4b2b-a776-637e864a08b6",
   "metadata": {
    "id": "3cd95ec2-6a59-4b2b-a776-637e864a08b6",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task 1 - Preprocessing <a class=\"anchor\" id=\"task1\"></a>\n",
    "[Link to notebook](1_preprocess.ipynb)  \n",
    "[Link to html](1_preprocess.html)\n",
    "\n",
    "Feature list of dataset\n",
    "* age : 5-years gap\n",
    "* height(cm)\n",
    "* weight(kg)\n",
    "* waist(cm) : Waist circumference length\n",
    "* eyesight(left)\n",
    "* eyesight(right)\n",
    "* hearing(left)\n",
    "* hearing(right)\n",
    "* systolic : Blood pressure\n",
    "* relaxation : Blood pressure\n",
    "* fasting blood sugar\n",
    "* Cholesterol : total\n",
    "* triglyceride\n",
    "* HDL : cholesterol type\n",
    "* LDL : cholesterol type\n",
    "* hemoglobin\n",
    "* Urine protein\n",
    "* serum creatinine\n",
    "* AST : glutamic oxaloacetic transaminase type\n",
    "* ALT : glutamic oxaloacetic transaminase type\n",
    "* Gtp : γ-GTP\n",
    "* dental caries\n",
    "\n",
    "Label:\n",
    "* smoking: label for this dataset. 0 = non-smoker, and 1 = smoker\n",
    "\n",
    "The distribution of our labels was slightly uneven - we had more non smokers than smokers in both train/test.\n",
    "\n",
    "<img src=\"images/1_preprocess/label_distribution.png\" alt=\"image\" style=\"height: 400px;\"/>\n",
    "\n",
    "We had a total of 38,984 records and 22 features. After reviewing the dataset we did not see any need to convert our categorical features to dummies. So we went right into doing a train/test split of 80/20. We decided to normalize our data with the scikit-learn's MinMax Scaler.\n",
    "\n",
    "Once this was completed we fit the data on our training set and transformed our training/test sets.  \n",
    "\n",
    "Finally we exported the data so it could be accessible for other steps. However, each team member also had the flexibility of doing their own preprocessing if they wanted in order to best work with their model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d056196-977d-4866-adb3-5a3f64c790dd",
   "metadata": {
    "id": "7d056196-977d-4866-adb3-5a3f64c790dd"
   },
   "source": [
    "## Task 2 - Exploratory Data Analysis <a class=\"anchor\" id=\"task2\"></a>\n",
    "[Link to notebook](2_0_exploratory_data_analysis.ipynb)  \n",
    "[Link to html](2_0_exploratory_data_analysis.html)  \n",
    "\n",
    "Our broad goal for Exploratory Data Analysis was to become more familiar with the values in our dataset. We set out to achieve that by looking at things like descriptive statistics, distributions, and correlations.  \n",
    "\n",
    "The first thing we checked was the data types of the 23 variables in our dataset. All of the variables were assigned numeric data types (float and int) when they were read into our notebook from the external file.\n",
    "\n",
    "<img src=\"images/2_eda/dtypes.png\" alt=\"image\"  style=\"height: 350px;\"/>\n",
    "\n",
    "While all fields had numerical values, we knew that there were some categorical variables in the dataset. So we performed further analysis to identify the categorical variables.  \n",
    "\n",
    "The next thing we checked was the descriptive statistics for all of the variables. There are two main takeaways from the descriptive statistics: the dataset appears to have five categorical variables (including our target variable), and there are significant differences in the ranges of values of our variables.  \n",
    "\n",
    "We will need to be mindful of the categorical variables when conducting further analysis and interpreting results. We should also normalize (or scale) our data. This will prevent some of the larger values we see from skewing our results.  \n",
    "\n",
    "Next, we generated box plots and histograms to review the distribution of values for each variable.\n",
    "\n",
    "<img src=\"images/2_eda/histogram1.png\" alt=\"image\"  style=\"height: 500px;\"/>\n",
    "<img src=\"images/2_eda/histogram2.png\" alt=\"image\"  style=\"height: 500px;\"/>\n",
    "<img src=\"images/2_eda/histogram3.png\" alt=\"image\"  style=\"height: 350px;\"/>\n",
    "\n",
    "Most of the distributions are normal. Some are slightly skewed, but nothing that is too concerning.  \n",
    "\n",
    "A few of the categorical variables in our data set are easy to spot when looking at the histograms like hearing (left), hearing (right), and dental caries.  \n",
    "\n",
    "Our target variable, smoking, is also categorical. There are two possible classes: non-smoker (0) and smoker (1). For this variable, we looked at the value counts to see the size of each class.  \n",
    "\n",
    "<img src=\"images/2_eda/counts.png\" alt=\"image\"  style=\"height: 150px;\"/>\n",
    "\n",
    "To better understand how the variables in our dataset are related to each other, we looked at the Pearson correlation coefficients for each pair of variables. We visualized these values in a correlation matrix to make it is easier to interpret.  \n",
    "\n",
    "<img src=\"images/2_eda/correlation.png\" alt=\"image\"  style=\"height: 400px;\"/>\n",
    "\n",
    "There are some intuitive correlations in our dataset like the positive correlations between: height and weight, weight and waist, systolic and relaxation (both heart rate measures), cholesterol and LDL, and AST and ALT (both glutamic oxaloacetic transaminase types).  \n",
    "\n",
    "We also see a negative correlation between age and height, which makes sense because humans become shorter in old age and our dataset contains observations for people up to 85 years old.  \n",
    "\n",
    "The variable HDL (sometimes referred to as the “good” cholesterol) has negative correlations with weight, waist, and triglyceride (a type of fat found in the blood).\n",
    "\n",
    "After conducting a correlation analysis, we proceeded with data grouping analysis. Initially, we employed K-means clustering and aimed to determine the optimal number of clusters (k). To accomplish this, we utilized both the silhouette score and the elbow method. The silhouette score suggested that either 2 or 3 clusters could be optimal, with 3 clusters exhibiting a lower within-cluster sum of squares. However, the elbow method indicated that the optimal k value was 4, as observed from the graph where the elbow occurred at k=4.\n",
    "\n",
    "Interestingly, the optimal number of clusters coincided with the number of labels. Consequently, we opted for 2 clusters to explore whether data points within clusters shared the same label. However, assessing homogeneity scores revealed that neither 2 nor 3 clusters effectively separated the data into distinct clusters corresponding to different classes or labels, as both scenarios yielded homogeneity scores below 0.01.\n",
    "\n",
    "<img src=\"images/2_eda/kmeans_graph.png\" alt=\"image\"  style=\"height: 400px;\"/>\n",
    "\n",
    "Subsequently, we employed Principal Component Analysis (PCA) to visualize the data in a 2D plane. This involved selecting 2 principal components and performing PCA on the scaled dataset. Upon plotting the principal components along with their labels, we observed clustering of data points, although the labels appeared to be mixed. Notably, while the data seemed linearly separable, utilizing the two principal components in Support Vector Machine (SVM) might not yield accurate results due to the blending of labels.\n",
    "\n",
    " <img src=\"images/2_eda/pca_clust.png\" alt=\"image\"  style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e41989-6ab4-49ad-a5c1-ca86b0ba7dbf",
   "metadata": {
    "id": "c2e41989-6ab4-49ad-a5c1-ca86b0ba7dbf"
   },
   "source": [
    "## Task 3 - Benchmark Model <a class=\"anchor\" id=\"task3\"></a>\n",
    "[Link to notebook](3_benchmark_model.ipynb)  \n",
    "[Link to html](3_benchmark_model.html)  \n",
    "\n",
    "We decided that we would create a benchmark model for our main models in task 4 to compare to.\n",
    "\n",
    "We used a decision tree for our model because of it's relative simplicity, the ability to visualize the decision tree, and the ability to calculate the feature importance.\n",
    "\n",
    "We started out by creating a basic model with no parameters. This resulted in a training accuracy of 100% and test accuracy of about 74%. Clearly this was overfitting so we decided to use grid search with cross validation.\n",
    "\n",
    "<img src=\"images/3_benchmark_model/gridsearch.png\" alt=\"image\"  style=\"height: 300px;\"/>\n",
    "\n",
    "With the parameters found in the grid search we got a model with 73% train and test accuracy.\n",
    "\n",
    "<img src=\"images/3_benchmark_model/feature_importance.png\" alt=\"image\" style=\"height: 600px;\"/>\n",
    "\n",
    "Looking at precision/recall we noticed the recall was much lower for smokers.\n",
    "\n",
    "Finally, the top 3 features of the model were:\n",
    "* height\n",
    "* Gtp\n",
    "* hemoglobin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daf4559-88c3-4a25-b884-6d90f69c9143",
   "metadata": {
    "id": "7daf4559-88c3-4a25-b884-6d90f69c9143"
   },
   "source": [
    "## Task 4 - Models <a class=\"anchor\" id=\"task4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05093df-1257-4990-bc7d-2502010f8bc3",
   "metadata": {
    "id": "c05093df-1257-4990-bc7d-2502010f8bc3"
   },
   "source": [
    "### Xgboost <a class=\"anchor\" id=\"xgboost\"></a>\n",
    "[Link to notebook](4_modeling_xgboost.ipynb)  \n",
    "[Link to html](4_modeling_xgboost.html)  \n",
    "Created by Matt.  \n",
    "\n",
    "I decided to use xgboost for my classifier based on it's speed and performance on a wide range of machine learning tasks. Xgboost is not in the scikit-learn package so we needed to add xgboost to our package manager.\n",
    "\n",
    "I used the normalized data from the preprocessing step to start out with my model.  \n",
    "\n",
    "I started out by creating a Xgboost model with no parameters to see what the training/test accuracy would be. This resulted in a training/test accuracy of 87% and 77% respectfully. Pretty good start compared to our benchmark model. However, when looking at the feature importance I noticed that height was the most important feature.\n",
    "\n",
    "<img src=\"images/4_modeling_xgboost/base_feature_importance.png\" alt=\"image\" style=\"height: 400px;\"/>\n",
    "\n",
    "I didn't think this made sense in the context of the problem we are trying to solve - height shouldn't have any importance when trying to determine if someone is a smoker or not - so I decided to remove it from the feature list and test the model again.\n",
    "\n",
    "Our test score dropped about 1% but that is a good trade off for a more generalized model.\n",
    "\n",
    "Next, I used gridsearch, with cross validation, to find the best features.  \n",
    "\n",
    "<img src=\"images/4_modeling_xgboost/gridsearch.png\" alt=\"image\" style=\"height: 300px;\"/>\n",
    "\n",
    "We got a tigher split between train/test and our test accuracy stayed consistent around 76.5%\n",
    "\n",
    "Best Parameters:\n",
    "* colsample_bytree = 0.8  \n",
    "* gamma = 0  \n",
    "* learning_rate = 0.1  \n",
    "* max_depth = 5  \n",
    "* min_child_weight = 1  \n",
    "* n_estimators = 1000  \n",
    "* reg_alpha = 0  \n",
    "* reg_lambda = 0  \n",
    "* subsample = 0.9\n",
    "\n",
    "Finally, I attempted to use feature selection, with the parameters from gridsearch, to see if this would have any impact on the model. I incremented the feature selection by 5% intervals.\n",
    "\n",
    "The best model removed the following features: hearing(left), Urine protein, hearing(right). This represents 86% of the features and leaves us with a total of 18 features (height had already been removed).\n",
    "\n",
    "<img src=\"images/4_modeling_xgboost/feature_selection_results.png\" alt=\"image\" style=\"height: 600px;\"/>\n",
    "\n",
    "We were able to increase the testing score with feature selection up to 77% and remained a less than 10% difference between train/test. Based on this producing the highest testing score and having a good trade off between bias/variance I decided to use this as the best model for xgboost.\n",
    "\n",
    "Best Xgboost Model:\n",
    "- Used gridsearch to find best parameters - see parameters above\n",
    "- Use feature selection to select 86% of features -> dropping height, hearing(left), Urine protein, hearing(right)\n",
    "- Traing accuracy = 85%\n",
    "- Test accuracy = 77%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8304d5a-cc33-484d-b241-9f1584488241",
   "metadata": {
    "id": "d8304d5a-cc33-484d-b241-9f1584488241"
   },
   "source": [
    "### Knn <a class=\"anchor\" id=\"knn\"></a>\n",
    "[Link to notebook](4_modeling_knn.ipynb)  \n",
    "[Link to html](4_modeling_knn.html)  \n",
    "Create by Jack.  \n",
    "\n",
    "Since the goal of our project is to create models that can accurately predict a categorical variable, I chose to create a K-Nearest Neighbors classifier.\n",
    "To start this task, I defined a KNN model using the default parameters from the KNeighborsClassifer() function in scikit-learn. This will serve as the baseline KNN model.\n",
    "\n",
    "\n",
    "<img src=\"images/4_modeling_knn/base_test_report.png\" alt=\"image\" style=\"height: 250px;\"/>\n",
    "\n",
    "The baseline model accurately predicted the target class for 72% of the observations from the test data we set aside during preprocessing.\n",
    "Next, I used GridSearch with 5-fold cross-validation to try and identify the best values to use for the parameters n_neighbors (k) and weights.  \n",
    "\n",
    "<img src=\"images/4_modeling_knn/gridsearch.png\" alt=\"image\" style=\"height: 300px;\"/>\n",
    "\n",
    "GridSearch determined that k = 82 and distance weights were the best parameters.\n",
    "To go one step further, I also ran cross-validation using the same combinations of possible parameters to see if I would reach the same conclusion as GridSearch.  \n",
    "\n",
    "It should come as no surprise that my cross-validation yielded the same results as GridSearch. The highest accuracy we recorded was 77.5%, which corresponded with the model that used k= 82 and distance weights.\n",
    "\n",
    "<img src=\"images/4_modeling_knn/cv_acc.png\" alt=\"image\" style=\"height: 350px;\"/>\n",
    "\n",
    "One insight that was gained from my cross-validation was that the accuracy noticeably levels off once we reach k = 30. The additional increases in accuracy are minimal. If we wanted to use a smaller value of k for any reason, we would not be sacrificing much accuracy.  \n",
    "\n",
    "Next, I wanted to see how the model would perform after applying a feature reduction technique. For this, I used Principal Component Analysis.\n",
    "After calculating the principal components, I reviewed the explained variance ratios\n",
    "\n",
    "<img src=\"images/4_modeling_knn/pca_variance.png\" alt=\"image\" style=\"height: 100px;\"/>\n",
    "\n",
    "There does appear to be a knee after the fourth component. However, I wanted to select a number of components that would explain a greater ratio of the variance in the dataset. So I decided to choose the first eight components, which in total explain about 91.6% of the variance.  \n",
    "\n",
    "I transformed the normalized test dataset using the eight principal components and then fit a new KNN model (using the best parameters identified by GridSearch) to the transformed data.  \n",
    "\n",
    "I applied the same transformation to the set aside test data and then ran the model against the transformed test data.\n",
    "\n",
    "<img src=\"images/4_modeling_knn/pca_test_preds.png\" alt=\"image\" style=\"height: 250px;\"/>\n",
    "\n",
    "The model accurately predicted the target class for 70% of the observations in the test data. This was just slightly worse than our baseline model (72% accuracy) and not far away from our best accuracy we saw during cross-validation (77.5%). This is promising performance considering this model was using a dataset with fewer dimensions.  \n",
    "\n",
    "Lastly, I defined our best KNN model based on all the analysis I completed.  \n",
    "\n",
    "After defining the model and fitting it to the normalized training data, I then ran it against the set aside test data.  \n",
    "\n",
    "<img src=\"images/4_modeling_knn/test_class.png\" alt=\"image\" style=\"height: 250px;\"/>\n",
    "\n",
    "The best model accurately predicted the target class for 79% of the observations in the test data. This was even better than the best accuracy we recorded during cross-validation.  \n",
    "\n",
    "<img src=\"images/4_modeling_knn/matrix.png\" alt=\"image\" style=\"height: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffecc425-8d90-4eaf-99c8-fa7c58d4a8db",
   "metadata": {
    "id": "ffecc425-8d90-4eaf-99c8-fa7c58d4a8db"
   },
   "source": [
    "### SVM <a class=\"anchor\" id=\"svm\"></a>\n",
    "[Link to notebook](4_modeling_svm.ipynb)  \n",
    "[Link to html](4_modeling_svm.html)  \n",
    "Created by Francisco.\n",
    "\n",
    "In my analysis, I explored Support Vector Machine (SVM) models, specifically focusing on both Linear and Radial Basis Function (RBF) kernels, particularly after observing the 2d plane plot of our dataset in Exploratory Data Analysis (EDA). The plot showed that the data was able to be split, just not with 2 principal components. There was a posibility that the variance lost could have caused the labels being mixed together. Because of this I wanted to explore if a svm model could fine this split using the original features and not the principal components.\n",
    "\n",
    "#### Linear SVM <a class=\"anchor\" id=\"lin_svm\"></a>\n",
    "\n",
    "Starting with a Linear SVM, I established a baseline model with default settings using sklearn, achieving a validation accuracy of 73%. This performance was comparable to our benchmark model. Next, I fine-tuned the Linear SVM using grid search with cross-validation, focusing on the regularization parameter (C). This adjusts the penalty for misclassification, with smaller values promoting a smoother decision boundary and potentially better generalization, while larger values aim for stricter classification of training data, which may lead to overfitting.\n",
    "Through this process, I discovered that a C value of 10 yielded optimal results, balancing between bias and variance.\n",
    "\n",
    " <img src=\"images/4_modeling_svm/linear_svm_gridsearch.png\" alt=\"image\"  style=\"height: 400px;\"/>\n",
    "\n",
    "Using this parameter, the new fine tuned model did ~0.01 better in accuracy on the validation split. Secondly, both the validation and training split received a similiar accuracy score so there was no sign of overfitting. Moreover, I analyzed feature importance in the Linear SVM, identifying top influential features such as ALT, Gtp, and Hemoglobin. Utilizing this insight, we retrained the model with feature reduction, dropping less impactful features. However, this led to a slight decrease in accuracy to 72% which is worse than our benchmark model. Given this, I decided to explore alternative SVM kernels, considering the potential for non-linear decision boundaries.\n",
    "\n",
    " <img src=\"images/4_modeling_svm/linear_svm_ft_imp.png\" alt=\"image\"  style=\"height: 400px;\"/>\n",
    "\n",
    "\n",
    "#### RBF SVM <a class=\"anchor\" id=\"rbf_svm\"></a>\n",
    "\n",
    "\n",
    "Moving on to RBF SVM, I observed promising initial results with a baseline accuracy of 74% on the validation split. This kernel was chosen due to its similarity to KNN and its robustness to outliers. Subsequently, we fine-tuned the RBF SVM using grid search with cross validation, focusing on both the regularization parameter (C) and the kernel coefficient (gamma). The best parameters were C=10 and gamma='scale'. Scale means gamma is dynamically adjusted based on the number of features and their variance effectively ensuring that the scale of gamma is appropriate relative to the data. Using these two parameters, the new fine tuned model did ~1% better in accuracy on the validation split.\n",
    "\n",
    " <img src=\"images/4_modeling_svm/rbf_svm_gridsearch.png\" alt=\"image\"  style=\"height: 400px;\"/>\n",
    "\n",
    "Upon validating the model on the test split, I obtained a consistent accuracy score of 74%, indicating robust performance without signs of overfitting. Additionally, the model demonstrated aptitude in capturing underlying patterns within the data, as evidenced by its performance on both training and test splits.\n",
    "\n",
    " <img src=\"images/4_modeling_svm/rbf_svm_cm.png\" alt=\"image\"  style=\"height: 400px;\"/>\n",
    "\n",
    "From the confusion matrix, we can see that my best model (RBF SVM) has a overall better performance in predicting non-smokers(0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af508a7f-1983-4f45-aba4-3d8d1bc47abf",
   "metadata": {
    "id": "af508a7f-1983-4f45-aba4-3d8d1bc47abf"
   },
   "source": [
    "### Evaluation <a class=\"anchor\" id=\"evaluation\"></a>\n",
    "[Link to notebook](4_modeling_combine_all_models.ipynb)  \n",
    "[Link to html](4_modeling_combine_all_models.html)  \n",
    "\n",
    "After completing our model building process, we conducted a comparative analysis of the performance of each model. All three models we developed exhibited superior accuracy scores compared to our benchmark model. Notably, the K-Nearest Neighbors (KNN) model outperformed the others, while the XGBoost model displayed slight indications of overfitting.\n",
    "\n",
    "\n",
    "| Model                         | Train Score | Test Score |\n",
    "|-------------------------------|-------------|------------|\n",
    "| Decision Tree (Benchmark)     | 0.729       | 0.73       |\n",
    "| KNN                           | 0.775       | 0.79       |\n",
    "| RBF SVM                       | 0.758       | 0.74       |\n",
    "| XGBoost                       | 0.857       | 0.772      |\n",
    "\n",
    ">Note: Accuracy scores are presented\n",
    "\n",
    "Additionally, we observed that the SVM and XGBoost models identified different patterns as the most important features. Recognizing this divergence, we explored the potential of combining our models to leverage their diverse pattern capturing abilities. The aim was to create a comprehensive ensemble model that merges the strengths of each individual model, potentially leading to enhanced overall performance.\n",
    "\n",
    "#### Ensemble Classifier <a class=\"anchor\" id=\"en_class\"></a>\n",
    "\n",
    "Since our task involved classification, we opted for either a Voting Classifier or a Stacking Classifier. Here's a clearer distinction between the two:\n",
    "\n",
    "- **Voting Classifier**:\n",
    "  - Offers both hard and soft voting variants.\n",
    "  - Hard voting selects the majority vote as the final prediction, while soft voting averages the probability scores predicted by individual models.\n",
    "- **Stacking Classifier**:\n",
    "  - Utilizes the outputs of individual models as inputs for another model, typically referred to as the \"stacked model\".\n",
    "  - We employed logistic regression with an L2 penalty as our stacked model.\n",
    "\n",
    "We tested these approaches to determine the most effective ensemble method:\n",
    "\n",
    "| Model                         | Train Score | Validation Score |\n",
    "|-------------------------------|-------------|------------|\n",
    "| Hard Voting Classifier     | 0.776       | 0.788      |\n",
    "| Soft Voting Classifier                           | 0.783       | 0.796       |\n",
    "| Stacking Classifier                      | 0.779       | 0.794       |\n",
    ">Note: Accuracy scores are presented\n",
    "\n",
    "Our analysis revealed that the Soft Voting Classifier emerged as the most effective ensemble model. It exhibited minimal signs of overfitting and achieved the highest accuracy score. Subsequently, we evaluated this model on the test split and obtained an accuracy score of 0.788, further affirming its robustness and generalization capability.\n",
    "\n",
    "#### Final Evaluation <a class=\"fin_evaluation\" id=\"en_class\"></a>\n",
    "\n",
    "After creating the ensemble classifier, we revisited all our models again.\n",
    "\n",
    "| Model                         | Train Score | Test Score |\n",
    "|-------------------------------|-------------|------------|\n",
    "| Decision Tree (Benchmark)     | 0.729       | 0.73       |\n",
    "| KNN                           | 0.775       | 0.79       |\n",
    "| RBF SVM                       | 0.758       | 0.74       |\n",
    "| XGBoost                       | 0.857       | 0.772      |\n",
    "| Soft Voting Classifier                           | 0.783       | 0.796       |\n",
    "\n",
    ">Note: Accuracy scores are presented\n",
    "\n",
    "Upon evaluation, it became evident that the Soft Voting Classifier outperformed all individual models, showcasing superior accuracy without any discernible signs of overfitting. This ensemble model effectively merged the strengths of each individual model, resulting in enhanced predictive performance.\n",
    "\n",
    " <img src=\"images/4_modeling_combine_all_models/best_model_cm.png\" alt=\"image\"  style=\"height: 400px;\"/>\n",
    "\n",
    "\n",
    "```\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "  non-smoker       0.83      0.84      0.83      4933\n",
    "      smoker       0.72      0.69      0.71      2864\n",
    "\n",
    "    accuracy                           0.79      7797\n",
    "   macro avg       0.77      0.77      0.77      7797\n",
    "weighted avg       0.79      0.79      0.79      7797\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8cd5f9-7e5b-4476-8ece-b15464740875",
   "metadata": {
    "id": "0b8cd5f9-7e5b-4476-8ece-b15464740875"
   },
   "source": [
    "## Task 5 - Application <a class=\"anchor\" id=\"task5\"></a>\n",
    "[Link to app](https://huggingface.co/spaces/FranciscoLozDataScience/smoker_model)\n",
    "\n",
    "Once we determined our best model, we embarked on creating our application. Our objective was to design a user-friendly interface where users could interact with the model's inputs and observe the predicted labels it generated. To achieve this, we turned to Gradio for its seamless configuration for creating AI apps, and HuggingFace for its integration with Gradio and its capability for free hosting.\n",
    "\n",
    "In creating the app, we employed the Python library joblib to save our best model and our min-max scaler. This approach eliminated the need for retraining the model each time the app was accessed.\n",
    "\n",
    "This application serves as a valuable tool for epidemiologists, enabling them to comprehend population health trends and information even when direct access to an individual's smoking status is unavailable. Additionally, it can be utilized to analyze older historical data where smoking status is unknown, and contacting individuals included in the dataset is not feasible.\n",
    "\n",
    " <img src=\"images/other/app_ui.png\" alt=\"image\"  style=\"height: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de90bd5-3af3-4466-9a20-9236339a4366",
   "metadata": {
    "id": "fadfa0b4-0a2c-4ced-b709-f5f943d56cf9"
   },
   "source": [
    "## Conclusion <a class=\"anchor\" id=\"conclusion\"></a>\n",
    "\n",
    "Wrapping up our project.  \n",
    "\n",
    "The problem we attempted to solve is being able to classify smoker's based on 22 bio signals. The dataset was available through kaggle.\n",
    "\n",
    "We wanted to be able to create a model that would be used in healthcare.  \n",
    "\n",
    "All group members learned how powerful ensemble models can be, and especially when the different models are capturing different patterns - which we saw with feature importance.\n",
    "\n",
    "One application for this type of model would be for epidemiologists to use this model when creating studies or looking at population health trends. If they didn't have access to whether or not a person was a smoker they'd be able to use our model for that classification.  \n",
    "\n",
    "Another application, would be for health insurance companies. They could use our model to help with premiums and risk. The premium and risk of a smoker vs non smoker should be different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d66823-7791-47c8-8fb6-7006734f336f",
   "metadata": {
    "id": "99d66823-7791-47c8-8fb6-7006734f336f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
